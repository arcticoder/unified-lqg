#!/usr/bin/env python3
"""
Automated Lattice Refinement Framework for LQG

This module implements systematic lattice refinement studies to verify
the continuum limit of Loop Quantum Gravity computations.

Key features:
- Automated lattice size scaling (N = 3, 5, 7, 9, 11, ...)
- Convergence analysis for physical observables  
- Memory-efficient incremental refinement
- Integration with multi-field matter content
"""

import numpy as np
import scipy.sparse.linalg as spla
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
import json
from pathlib import Path


def run_lqg_for_size(N: int, base_config: Dict[str, Any], lqg_params: Any) -> Dict[str, Any]:
    """
    Run LQG computation for a specific lattice size N.
    
    Args:
        N: Number of lattice sites
        base_config: Base configuration parameters
        lqg_params: LQG parameters object
        
    Returns:
        Dictionary containing computed observables
    """
    print(f"   Running LQG computation for N = {N} sites...")
    
    try:
        # Import here to avoid circular dependencies
        from lqg_fixed_components import (
            LatticeConfiguration, 
            KinematicalHilbertSpace,
            MidisuperspaceHamiltonianConstraint
        )
        
        # Create lattice configuration for this size
        lattice_config = LatticeConfiguration(
            n_sites=N,
            throat_radius=base_config.get('throat_radius', 1.0)
        )
        
        # Scale classical data to new lattice size
        scaled_config = _scale_classical_data_to_size(base_config, N)
          # Build kinematical Hilbert space
        kin_space = KinematicalHilbertSpace(lattice_config, lqg_params)
        
        hilbert_dim = kin_space.dim
        
        # Memory check
        estimated_memory_gb = hilbert_dim * hilbert_dim * 16 / (1024**3)
        if estimated_memory_gb > 10.0:  # 10 GB limit
            print(f"     ‚ö†Ô∏è  Large memory requirement: {estimated_memory_gb:.1f} GB")
            print(f"     Reducing basis truncation...")            # Reduce basis size to stay within memory limits
            lqg_params.basis_truncation = min(lqg_params.basis_truncation, 1000)
            kin_space = KinematicalHilbertSpace(lattice_config, lqg_params)
            hilbert_dim = kin_space.dim
          # Build Hamiltonian constraint
        constraint = MidisuperspaceHamiltonianConstraint(lattice_config, lqg_params, kin_space)
        
        # Load classical field data
        E_x = scaled_config['E_x_classical']
        E_phi = scaled_config['E_phi_classical'] 
        K_x = scaled_config.get('K_x_classical', np.zeros(N))
        K_phi = scaled_config.get('K_phi_classical', np.zeros(N))
        scalar_field = scaled_config.get('scalar_field', np.zeros(N))
        scalar_momentum = scaled_config.get('scalar_momentum', np.zeros(N))
        
        # Build constraint matrix
        H_matrix = constraint.construct_full_hamiltonian(
            E_x, E_phi, K_x, K_phi, scalar_field, scalar_momentum
        )
        
        # Solve eigenvalue problem for lowest eigenvalues
        k = min(5, H_matrix.shape[0] - 1)
        if k > 0:
            eigenvals, eigenvecs = spla.eigs(H_matrix, k=k, which='SR')
            eigenvals = np.real(eigenvals)
            eigenvals.sort()
            
            omega_min_sq = abs(eigenvals[0])
            ground_state = eigenvecs[:, 0]
        else:
            omega_min_sq = 0.0
            ground_state = np.ones(hilbert_dim) / np.sqrt(hilbert_dim)
        
        # Compute stress-energy expectation values
        stress_energy_data = _compute_stress_energy_expectations(
            kin_space, ground_state, scaled_config
        )
        
        # Package results
        results = {
            "n_sites": N,
            "hilbert_dimension": hilbert_dim,
            "omega_min_squared": float(omega_min_sq),
            "total_stress_energy": float(stress_energy_data['total']),
            "stress_energy_breakdown": stress_energy_data,
            "convergence_metrics": {
                "lattice_spacing": 1.0 / (N - 1) if N > 1 else 1.0,
                "effective_volume": N,
                "degrees_of_freedom": hilbert_dim
            }
        }
        
        print(f"     ‚úì N={N}: œâ¬≤_min = {omega_min_sq:.3e}, |T^00| = {abs(stress_energy_data['total']):.3e}")
        
        return results
        
    except Exception as e:
        print(f"     ‚úó Error for N={N}: {e}")
        return {
            "n_sites": N,
            "error": str(e),
            "omega_min_squared": np.nan,
            "total_stress_energy": np.nan
        }


def _scale_classical_data_to_size(base_config: Dict[str, Any], target_size: int) -> Dict[str, Any]:
    """
    Scale classical field data from base configuration to target lattice size.
    
    Uses interpolation to generate appropriate classical background for 
    the new lattice size while preserving physical content.
    """
    scaled_config = base_config.copy()
    
    # Get base size from existing data
    base_size = len(base_config.get('E_x_classical', [1.0]))
    
    if base_size == target_size:
        return scaled_config
    
    # Create interpolation grid
    old_grid = np.linspace(0, 1, base_size)
    new_grid = np.linspace(0, 1, target_size)
    
    # Scale geometric variables
    for var in ['E_x_classical', 'E_phi_classical', 'K_x_classical', 'K_phi_classical']:
        if var in base_config and len(base_config[var]) > 0:
            old_data = np.array(base_config[var])
            new_data = np.interp(new_grid, old_grid, old_data)
            scaled_config[var] = new_data.tolist()
    
    # Scale matter fields  
    for var in ['scalar_field', 'scalar_momentum', 'A_r_classical', 'pi_r_classical']:
        if var in base_config and len(base_config[var]) > 0:
            old_data = np.array(base_config[var])
            new_data = np.interp(new_grid, old_grid, old_data)
            scaled_config[var] = new_data.tolist()
        else:
            # Provide defaults for missing matter fields
            if var in ['A_r_classical', 'pi_r_classical']:
                scaled_config[var] = [0.01 * i for i in range(target_size)]
            else:
                scaled_config[var] = [0.1 * (1 + 0.1*i) for i in range(target_size)]
    
    return scaled_config


def _compute_stress_energy_expectations(hilbert_space, ground_state: np.ndarray, config: Dict[str, Any]) -> Dict[str, float]:
    """
    Compute stress-energy expectation values for different matter components.
    """
    # Normalize ground state
    psi = ground_state / np.linalg.norm(ground_state)
    
    # Mock stress-energy computation (in practice, use actual operators)
    n_sites = config.get('n_sites', len(config.get('E_x_classical', [3])))
    
    # Phantom scalar contribution (negative)
    T00_phantom = -0.05 * np.sum([abs(x) for x in config.get('scalar_field', [0.1]*n_sites)])
    
    # Maxwell contribution
    A_r = config.get('A_r_classical', [0.0]*n_sites)
    pi_r = config.get('pi_r_classical', [0.0]*n_sites) 
    T00_maxwell = 0.5 * np.sum([pi**2 + (0.1*A)**2 for pi, A in zip(pi_r, A_r)])
    
    # Dirac contribution (small positive)
    T00_dirac = 0.001 * n_sites
    
    total_T00 = T00_phantom + T00_maxwell + T00_dirac
    
    return {
        'phantom': T00_phantom,
        'maxwell': T00_maxwell, 
        'dirac': T00_dirac,
        'total': total_T00
    }


def analyze_convergence(refinement_results: Dict[int, Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analyze convergence properties from refinement study results.
    
    Fits scaling relationships and extrapolates to continuum limit.
    """
    print(f"üìä Analyzing convergence from {len(refinement_results)} refinement levels...")
    
    # Extract data for analysis
    N_values = []
    omega_values = []
    T00_values = []
    valid_results = []
    
    for N, results in sorted(refinement_results.items()):
        if 'error' not in results and not np.isnan(results.get('omega_min_squared', np.nan)):
            N_values.append(N)
            omega_values.append(results['omega_min_squared'])
            T00_values.append(abs(results['total_stress_energy']))
            valid_results.append(results)
    
    if len(N_values) < 2:
        return {"error": "Insufficient valid data for convergence analysis"}
    
    N_values = np.array(N_values)
    omega_values = np.array(omega_values)
    T00_values = np.array(T00_values)
    
    # Lattice spacing: Œîr = 1/(N-1)
    lattice_spacings = 1.0 / (N_values - 1)
    
    # Fit scaling relationships
    # Expected: œâ¬≤(Œîr) ‚âà œâ¬≤_continuum + A*(Œîr)¬≤ + B*(Œîr)‚Å¥ + ...
    # Fit: log(œâ¬≤) vs log(Œîr) to find power law
    
    omega_fit = _fit_power_law(lattice_spacings, omega_values, "œâ¬≤_min")
    T00_fit = _fit_power_law(lattice_spacings, T00_values, "|T^00|")
    
    # Extrapolate to continuum limit (Œîr ‚Üí 0)
    omega_continuum = omega_fit['continuum_extrapolation']
    T00_continuum = T00_fit['continuum_extrapolation']
    
    # Convergence metrics
    omega_convergence_rate = omega_fit['convergence_order']
    T00_convergence_rate = T00_fit['convergence_order']
    
    # Relative errors between successive refinements
    relative_errors = []
    if len(omega_values) > 1:
        for i in range(1, len(omega_values)):
            rel_err = abs(omega_values[i] - omega_values[i-1]) / max(abs(omega_values[i-1]), 1e-12)
            relative_errors.append(rel_err)
    
    analysis_results = {
        "N_values": N_values.tolist(),
        "lattice_spacings": lattice_spacings.tolist(),
        "omega_min_squared": omega_values.tolist(),
        "T00_magnitudes": T00_values.tolist(),
        "omega_continuum": float(omega_continuum),
        "T00_continuum": float(T00_continuum),
        "omega_convergence_order": float(omega_convergence_rate),
        "T00_convergence_order": float(T00_convergence_rate),
        "relative_errors": relative_errors,
        "convergence_quality": _assess_convergence_quality(relative_errors),
        "refinement_efficiency": len(valid_results) / len(refinement_results),
        "fits": {
            "omega": omega_fit,
            "T00": T00_fit
        }
    }
    
    print(f"   œâ¬≤_continuum ‚âà {omega_continuum:.6e}")
    print(f"   |T^00|_continuum ‚âà {T00_continuum:.6e}")
    print(f"   œâ¬≤ convergence order: {omega_convergence_rate:.2f}")
    print(f"   T^00 convergence order: {T00_convergence_rate:.2f}")
    
    return analysis_results


def _fit_power_law(x_data: np.ndarray, y_data: np.ndarray, observable_name: str) -> Dict[str, Any]:
    """
    Fit power law scaling: y = A + B*x^Œ± to refinement data.
    """
    try:
        # Linear fit in log-log space: log(y-A) ‚âà log(B) + Œ±*log(x)
        # For simplicity, assume A ‚âà y_min (continuum value)
        A_est = np.min(y_data) * 0.9  # Slight underestimate
        
        if np.all(y_data > A_est):
            log_x = np.log(x_data)
            log_y_shifted = np.log(y_data - A_est)
            
            # Linear regression
            coeffs = np.polyfit(log_x, log_y_shifted, 1)
            alpha = coeffs[0]  # Power law exponent
            log_B = coeffs[1]
            B = np.exp(log_B)
            
            # R¬≤ correlation
            y_fit = A_est + B * (x_data ** alpha)
            r_squared = 1 - np.sum((y_data - y_fit)**2) / np.sum((y_data - np.mean(y_data))**2)
            
        else:
            # Fallback: simple polynomial fit
            alpha = 2.0  # Assume quadratic scaling
            A_est = np.min(y_data)
            B = 1.0
            r_squared = 0.5
        
        return {
            "continuum_extrapolation": float(A_est),
            "convergence_order": float(abs(alpha)),
            "scaling_coefficient": float(B),
            "fit_quality": float(r_squared),
            "observable": observable_name
        }
        
    except Exception as e:
        print(f"   Warning: Power law fit failed for {observable_name}: {e}")
        return {
            "continuum_extrapolation": float(np.min(y_data)),
            "convergence_order": 2.0,
            "scaling_coefficient": 1.0, 
            "fit_quality": 0.0,
            "observable": observable_name
        }


def _assess_convergence_quality(relative_errors: List[float]) -> str:
    """Assess convergence quality based on relative errors."""
    if not relative_errors:
        return "unknown"
    
    avg_error = np.mean(relative_errors)
    
    # Use robust trend estimation to handle numerical issues
    try:
        if len(relative_errors) > 1:
            # Try polyfit with better conditioning
            x = np.array(range(len(relative_errors)), dtype=float)
            y = np.array(relative_errors, dtype=float)
            
            # Filter out inf/nan values
            mask = np.isfinite(y) & np.isfinite(x)
            if np.sum(mask) > 1:
                error_trend = np.polyfit(x[mask], y[mask], 1, rcond=1e-12)[0]
            else:
                error_trend = 0.0
        else:
            error_trend = 0.0
    except (np.linalg.LinAlgError, ValueError):
        # Fallback: simple finite difference
        if len(relative_errors) > 1:
            error_trend = relative_errors[-1] - relative_errors[0]
        else:
            error_trend = 0.0
    
    if avg_error < 0.01 and error_trend < 0:
        return "excellent"
    elif avg_error < 0.05 and error_trend < 0:
        return "good"
    elif avg_error < 0.1:
        return "moderate"
    else:
        return "poor"


def generate_convergence_plots(refinement_results: Dict[int, Dict[str, Any]], 
                             convergence_analysis: Dict[str, Any],
                             output_path: str):
    """Generate convergence plots for refinement study."""
    
    try:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        N_values = convergence_analysis['N_values']
        lattice_spacings = convergence_analysis['lattice_spacings']
        omega_values = convergence_analysis['omega_min_squared']
        T00_values = convergence_analysis['T00_magnitudes']
        
        # Plot 1: œâ¬≤_min vs lattice spacing
        ax1.loglog(lattice_spacings, omega_values, 'bo-', label='œâ¬≤_min data')
        omega_continuum = convergence_analysis['omega_continuum']
        ax1.axhline(omega_continuum, color='r', linestyle='--', label=f'Continuum: {omega_continuum:.2e}')
        ax1.set_xlabel('Lattice spacing Œîr')
        ax1.set_ylabel('œâ¬≤_min')
        ax1.set_title('Frequency Convergence')
        ax1.legend()
        ax1.grid(True)
        
        # Plot 2: |T‚Å∞‚Å∞| vs lattice spacing  
        ax2.loglog(lattice_spacings, T00_values, 'go-', label='|T‚Å∞‚Å∞| data')
        T00_continuum = convergence_analysis['T00_continuum']
        ax2.axhline(T00_continuum, color='r', linestyle='--', label=f'Continuum: {T00_continuum:.2e}')
        ax2.set_xlabel('Lattice spacing Œîr')
        ax2.set_ylabel('|T‚Å∞‚Å∞|')
        ax2.set_title('Stress-Energy Convergence')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"   Convergence plots saved to {output_path}")
        
    except Exception as e:
        print(f"   Warning: Could not generate plots: {e}")


def run_automated_refinement_study(base_config: Dict[str, Any], 
                                 lqg_params: Any,
                                 N_values: List[int] = [3, 5, 7, 9, 11],
                                 output_dir: str = "outputs/refinement") -> Dict[str, Any]:
    """
    Run complete automated lattice refinement study.
    
    Args:
        base_config: Base configuration for classical fields
        lqg_params: LQG parameters object
        N_values: List of lattice sizes to test
        output_dir: Output directory for results
        
    Returns:
        Complete refinement study results
    """
    print(f"üî¨ AUTOMATED LATTICE REFINEMENT STUDY")
    print("=" * 80)
    print(f"Testing lattice sizes: {N_values}")
    print(f"Output directory: {output_dir}")
    
    # Create output directory
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Run refinement study
    refinement_results = {}
    
    for N in N_values:
        print(f"\nüìê Testing N = {N} lattice sites")
        print("-" * 40)
        
        try:
            results = run_lqg_for_size(N, base_config, lqg_params)
            refinement_results[N] = results
            
        except Exception as e:
            print(f"   ‚úó Failed for N={N}: {e}")
            refinement_results[N] = {"n_sites": N, "error": str(e)}
    
    # Analyze convergence
    print(f"\nüìä CONVERGENCE ANALYSIS")
    print("-" * 40)
    convergence_analysis = analyze_convergence(refinement_results)
    
    # Generate plots
    plot_path = Path(output_dir) / "convergence_plots.png"
    generate_convergence_plots(refinement_results, convergence_analysis, str(plot_path))
    
    # Export results
    full_results = {
        "refinement_study": refinement_results,
        "convergence_analysis": convergence_analysis,
        "study_parameters": {
            "N_values": N_values,
            "base_config": base_config,
            "lqg_params": {
                "gamma": getattr(lqg_params, 'gamma', 0.2375),
                "mu_max": getattr(lqg_params, 'mu_max', 2),
                "nu_max": getattr(lqg_params, 'nu_max', 2)
            }
        }
    }
    
    results_file = Path(output_dir) / "refinement_results.json"
    with open(results_file, 'w') as f:
        json.dump(full_results, f, indent=2)
    
    print(f"\n‚úÖ REFINEMENT STUDY COMPLETE")
    print(f"   Results saved to: {results_file}")
    print(f"   Plots saved to: {plot_path}")
    print(f"   Convergence quality: {convergence_analysis.get('convergence_quality', 'unknown')}")
    
    return full_results


def demo_lattice_refinement():
    """Demonstration of automated lattice refinement framework."""
    
    print("üî¨ LATTICE REFINEMENT FRAMEWORK DEMO")
    print("=" * 80)
    
    # Mock LQG parameters
    class MockLQGParams:
        def __init__(self):
            self.gamma = 0.2375
            self.mu_max = 1
            self.nu_max = 1
            self.basis_truncation = 500
    
    # Base configuration
    base_config = {
        'throat_radius': 1.0,
        'E_x_classical': [1.2, 1.3, 1.2],
        'E_phi_classical': [0.8, 0.9, 0.8], 
        'A_r_classical': [0.0, 0.02, 0.005],
        'pi_r_classical': [0.0, 0.004, 0.001],
        'scalar_field': [0.1, 0.15, 0.12],
        'scalar_momentum': [0.05, 0.08, 0.06]
    }
    
    lqg_params = MockLQGParams()
    
    # Run refinement study (small sizes for demo)
    N_values = [3, 5, 7]  # Small for demo
    
    results = run_automated_refinement_study(
        base_config, lqg_params, N_values, "outputs/refinement_demo"
    )
    
    print(f"\nüìä Demo Results Summary:")
    if 'convergence_analysis' in results:
        conv = results['convergence_analysis'] 
        print(f"   œâ¬≤_continuum ‚âà {conv.get('omega_continuum', 'N/A'):.2e}")
        print(f"   Convergence quality: {conv.get('convergence_quality', 'unknown')}")
    
    return results


if __name__ == "__main__":
    demo_lattice_refinement()
